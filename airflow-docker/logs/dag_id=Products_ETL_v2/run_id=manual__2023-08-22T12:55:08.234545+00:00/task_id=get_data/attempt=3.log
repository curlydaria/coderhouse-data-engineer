[2023-08-22 13:01:17,458] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: Products_ETL_v2.get_data manual__2023-08-22T12:55:08.234545+00:00 [queued]>
[2023-08-22 13:01:17,466] {taskinstance.py:1179} INFO - Dependencies all met for <TaskInstance: Products_ETL_v2.get_data manual__2023-08-22T12:55:08.234545+00:00 [queued]>
[2023-08-22 13:01:17,468] {taskinstance.py:1376} INFO - 
--------------------------------------------------------------------------------
[2023-08-22 13:01:17,472] {taskinstance.py:1377} INFO - Starting attempt 3 of 6
[2023-08-22 13:01:17,474] {taskinstance.py:1378} INFO - 
--------------------------------------------------------------------------------
[2023-08-22 13:01:17,539] {taskinstance.py:1397} INFO - Executing <Task(PythonOperator): get_data> on 2023-08-22 12:55:08.234545+00:00
[2023-08-22 13:01:17,555] {standard_task_runner.py:52} INFO - Started process 59108 to run task
[2023-08-22 13:01:17,575] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'Products_ETL_v2', 'get_data', 'manual__2023-08-22T12:55:08.234545+00:00', '--job-id', '119', '--raw', '--subdir', 'DAGS_FOLDER/myETL.py', '--cfg-path', '/tmp/tmp8guvqsyc', '--error-file', '/tmp/tmp2zs_z1vz']
[2023-08-22 13:01:17,580] {standard_task_runner.py:80} INFO - Job 119: Subtask get_data
[2023-08-22 13:01:17,680] {task_command.py:371} INFO - Running <TaskInstance: Products_ETL_v2.get_data manual__2023-08-22T12:55:08.234545+00:00 [running]> on host 0110d0e687e9
[2023-08-22 13:01:17,734] {taskinstance.py:1591} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Sol
AIRFLOW_CTX_DAG_ID=Products_ETL_v2
AIRFLOW_CTX_TASK_ID=get_data
AIRFLOW_CTX_EXECUTION_DATE=2023-08-22T12:55:08.234545+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-08-22T12:55:08.234545+00:00
[2023-08-22 13:01:17,774] {python.py:173} INFO - Done. Returned value was:                          _id  ...   createdBy_name
0   61ab420c0f34753bcedfa787  ...       Sofia Lily
1   61ab42600f34753bcedfa78b  ...        Mark Lyan
2   61ab42790f34753bcedfa78f  ...       Sofia Lily
3   61ab42d00f34753bcedfa79e  ...        Aria Ella
4   61ab42e90f34753bcedfa7a2  ...   William Counts
5   61ab431c0f34753bcedfa7a6  ...  Robert Gonzalez
6   61ab43350f34753bcedfa7aa  ...         Noah Ali
7   61ab434b0f34753bcedfa7ae  ...        Alex Dwan
8   61ab44f40f34753bcedfa819  ...   James Anderson
9   61ab450a0f34753bcedfa81d  ...   William Counts
10  61ab47110f34753bcedfa83f  ...      Steve Smith
11  61ab472c0f34753bcedfa843  ...  Robert Gonzalez
12  61ab475a0f34753bcedfa847  ...        Sajal Mia
13  61ab47c20f34753bcedfa853  ...      Nova Aurora
14  61ab47e70f34753bcedfa857  ...     David Miller
15  61ab47fd0f34753bcedfa85b  ...        Tom Hardy
16  61ab48350f34753bcedfa862  ...         Noah Ali
17  61ab484d0f34753bcedfa866  ...  Robert Gonzalez
18  61ab68a30f34753bcedfa898  ...        Sajal Mia
19  61ebbfe64efdea94da60a734  ...      Super Admin

[20 rows x 12 columns]
[2023-08-22 13:01:17,811] {xcom.py:586} ERROR - Could not serialize the XCom value into JSON. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your *** config.
[2023-08-22 13:01:17,813] {taskinstance.py:1909} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 2412, in xcom_push
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 198, in set
    map_index=map_index,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 583, in serialize_value
    return json.dumps(value).encode('UTF-8')
  File "/usr/local/lib/python3.7/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/local/lib/python3.7/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.7/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/local/lib/python3.7/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DataFrame is not JSON serializable
[2023-08-22 13:01:17,827] {taskinstance.py:1420} INFO - Marking task as UP_FOR_RETRY. dag_id=Products_ETL_v2, task_id=get_data, execution_date=20230822T125508, start_date=20230822T130117, end_date=20230822T130117
[2023-08-22 13:01:17,835] {standard_task_runner.py:97} ERROR - Failed to execute job 119 for task get_data (Object of type DataFrame is not JSON serializable; 59108)
[2023-08-22 13:01:17,872] {local_task_job.py:156} INFO - Task exited with return code 1
[2023-08-22 13:01:17,923] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
